# -*- coding: utf-8 -*-
"""Q_3Drf_logisticR_part_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10pvAVATASBoicjicRro35mE1DhwmGSKO

**Import the all necessary library**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
import nltk
import warnings
# %matplotlib inline

warnings.filterwarnings('ignore')

"""**Version of GPU**"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

"""**Import the Twitter Dataset**"""

input_file = '/content/Twitter_Data.csv'

"""**Converted Into DataFrame**"""

input_df=pd.read_csv(input_file)
input_df.head()

input_df

"""**find Shape of DataFrame**"""

input_df.shape

"""**check the Null values present in Data set**"""

input_df.isnull().sum()

"""**Remove null value**"""

input_df.dropna(inplace=True)

input_df.isnull().sum()

input_df.info()

"""**Our category column present Float value converted into int**"""

input_df['category'] =input_df['category'].astype(int)

"""**change the label**

*   List item positive(+1)--->postive(2)
*   List item Negative(-1)--->Negative(0)
*   List item Neutral(0)----->neutral(1)




"""

input_df['category'] = input_df['category'].replace({-1: 0, 0: 1, 1: 2})

input_df

null_check = input_df.isnull()
print(null_check.sum())

input_df.describe()

"""**check length of each clean_text**"""

input_df['clean_text'].apply(len)

"""**`below the and above 5 number clean_text in data set`**"""

sum(input_df['clean_text'].apply(len)>5),sum(input_df['clean_text'].apply(len)<5)

"""**Keep bigger then 5 word in clean_text**"""

input_df=input_df[input_df['clean_text'].apply(len)>5]
print(len(input_df))

"""**count the number of Label present in each category**"""

input_df['category'].value_counts()

def remove_pattern(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for word in r:
        input_txt = re.sub(word, "", input_txt)
    return input_txt

input_df.head()

"""**check word start with the @ because start of tweet everyone used @**"""

import re
def count_handles(text):
    return len(re.findall(r'@[\w]+', text))

input_df['handle_count'] = input_df['clean_text'].apply(count_handles)

input_df

"""**n0 @ found in any row**

**Check how special character,punchuation and numeric present in clean_text**
"""

def count_special_characters(text):
    special_chars = re.findall(r'[^a-zA-Z0-9\s]', text)
    return len(special_chars)

def count_punctuation(text):
    punctuation = re.findall(r'[^\w\s]', text)
    return len(punctuation)

def count_numbers(text):
    numbers = re.findall(r'[0-9]', text)
    return len(numbers)

input_df['special_char_count'] = input_df['clean_text'].apply(count_special_characters)
input_df['punctuation_count'] = input_df['clean_text'].apply(count_punctuation)
input_df['number_count'] = input_df['clean_text'].apply(count_numbers)

input_df

"""**count special character,punchuation and numeric present in clean_text**"""

total_handle_count = input_df['handle_count'].sum()
total_special_char_count = input_df['special_char_count'].sum()
total_punctuation_count = input_df['punctuation_count'].sum()
total_number_count = input_df['number_count'].sum()

print(f"Total Handle Count: {total_handle_count}")
print(f"Total Special Character Count: {total_special_char_count}")
print(f"Total Punctuation Count: {total_punctuation_count}")
print(f"Total Number Count: {total_number_count}")

"""**Now remove all  special character,punchuation and numeric**"""

def remove_special_punctuation_numbers(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    return text

input_df['clean_text_more'] = input_df['clean_text'].apply(remove_special_punctuation_numbers)

input_df

input_df['clean_tweet_'] = input_df['clean_text_more'].apply(lambda x: " ".join([w for w in x.split() if len(w)>3]))
input_df.head()

"""**Drop some extra column **"""

input_df = input_df.drop(['handle_count','special_char_count','punctuation_count','number_count'], axis=1)

input_df

input_df.columns

"""**Stemming apply like running---> run**"""

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()

def stem_text(text):
    words = word_tokenize(text)
    stemmed_words = [stemmer.stem(word) for word in words]
    return ' '.join(stemmed_words)

import nltk
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

nltk.download('punkt')

input_df['clean_tweet_'] = input_df['clean_tweet_'].apply(stem_text)

input_df

"""**load in excel file and applied to BERT Prediction**"""

import pandas as pd
output_file = '/content/modified_input_df.xlsx'
input_df.to_excel(output_file, index=False)

from google.colab import files
files.download(output_file)

!pip install wordcloud

input_df.isnull().sum()

"""**frequecy of word is hight then word look more bold**"""

all_words = " ".join([sentence for sentence in input_df['clean_tweet_']])

from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)

plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""**frequent words visualization for +ve**"""

all_words = " ".join([sentence for sentence in input_df['clean_tweet_'][input_df['category']==2]])

wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)

plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""**frequent words visualization for -ive**"""

all_words = " ".join([sentence for sentence in input_df['clean_tweet_'][input_df['category']==0]])

wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)

plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""**frequent words visualization for nuetral**"""

all_words = " ".join([sentence for sentence in input_df['clean_tweet_'][input_df['category']==1]])

wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)

plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

import matplotlib.pyplot as plt
from collections import Counter
from wordcloud import WordCloud

"""**Concatenate all 'clean_tweet_' texts where category is neutral**"""

import matplotlib.pyplot as plt
from collections import Counter
all_words = " ".join(input_df[input_df['category'] == 1]['clean_tweet_'])
words = all_words.split()
word_freq = Counter(words)
top_words = word_freq.most_common(20)
top_words, frequencies = zip(*top_words)
plt.figure(figsize=(12, 8))
plt.bar(top_words, frequencies, color='skyblue')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Top 20 Most Frequent Words in Negative Sentiment Tweets')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

all_words = " ".join(input_df[input_df['category'] == 0]['clean_tweet_'])
words = all_words.split()
word_freq = Counter(words)
top_words = word_freq.most_common(20)
top_words, frequencies = zip(*top_words)
plt.figure(figsize=(12, 8))
plt.bar(top_words, frequencies, color='skyblue')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Top 15 Most Frequent Words in Negative Sentiment Tweets')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**Concatenate all 'clean_tweet_' texts where category is +ive**"""

all_words = " ".join(input_df[input_df['category'] == 2]['clean_tweet_'])
words = all_words.split()
word_freq = Counter(words)
top_words = word_freq.most_common(15)
top_words, frequencies = zip(*top_words)
plt.figure(figsize=(12, 8))
plt.bar(top_words, frequencies, color='skyblue')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Top 15 Most Frequent Words in Negative Sentiment Tweets')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

from sklearn.feature_extraction.text import CountVectorizer
bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=100000, stop_words='english')
bow = bow_vectorizer.fit_transform(input_df['clean_tweet_'])

"""**splitting the data set train and test**"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(bow, input_df['category'], random_state=42, test_size=0.25)

"""**LogisticRegression method is used for training and Testing**

1. Bag of Words (Count Vectorizer)
2. TF-IDF Vectorizer
3. Word2Vec
4. FastText

**Logistic regression Method Use for training and Testing**
"""

import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from gensim.models import Word2Vec, FastText
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Assuming input_df and tokens are already defined as in your previous code

vectorizations = {}

# 1. Bag of Words (Count Vectorizer)
bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=100000, stop_words='english')
vectorizations['bow'] = bow_vectorizer.fit_transform(input_df['clean_tweet_'])

# 2. TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=100000, stop_words='english')
vectorizations['tfidf'] = tfidf_vectorizer.fit_transform(input_df['clean_tweet_'])

# 3. Word2Vec
word2vec_model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=2, workers=4)
word2vec_vectors = [np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for words in tokens]
vectorizations['word2vec'] = np.array(word2vec_vectors)

# 4. FastText
fasttext_model = FastText(sentences=tokens, vector_size=100, window=5, min_count=2, workers=4)
fasttext_vectors = [np.mean([fasttext_model.wv[word] for word in words if word in fasttext_model.wv] or [np.zeros(100)], axis=0) for words in tokens]
vectorizations['fasttext'] = np.array(fasttext_vectors)

# Define classifiers
logreg = LogisticRegression(max_iter=1000)
classifiers = [('Logistic Regression', logreg)]

# Train and evaluate each vectorization method with each classifier
for vec_name, vec_data in vectorizations.items():
    print(f"Vectorization: {vec_name}")
    x_train, x_test, y_train, y_test = train_test_split(vec_data, input_df['category'], random_state=42, test_size=0.25)

    for clf_name, clf in classifiers:
        clf.fit(x_train, y_train)
        pred = clf.predict(x_test)

        # Calculate evaluation metrics
        accuracy = accuracy_score(y_test, pred)
        precision = precision_score(y_test, pred, average='macro')
        recall = recall_score(y_test, pred, average='macro')
        f1 = f1_score(y_test, pred, average='macro')

        # Print metrics
        print(f"Classifier: {clf_name}")
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1-score: {f1:.4f}")

        # Generate confusion matrix
        cm = confusion_matrix(y_test, pred)
        print("Confusion Matrix:")
        print(cm)

        print()  # Separate different vectorization or classifier outputs

"""**Logistic regression Method Use for training and**"""

import pandas as pd
import numpy as np
from gensim.models import Word2Vec, FastText
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix



vectorizations = {}

bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=100000, stop_words='english')
vectorizations['bow'] = bow_vectorizer.fit_transform(input_df['clean_tweet_'])

tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=100000, stop_words='english')
vectorizations['tfidf'] = tfidf_vectorizer.fit_transform(input_df['clean_tweet_'])

word2vec_model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=2, workers=4)
word2vec_vectors = [np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for words in tokens]
vectorizations['word2vec'] = np.array(word2vec_vectors)

fasttext_model = FastText(sentences=tokens, vector_size=100, window=5, min_count=2, workers=4)
fasttext_vectors = [np.mean([fasttext_model.wv[word] for word in words if word in fasttext_model.wv] or [np.zeros(100)], axis=0) for words in tokens]
vectorizations['fasttext'] = np.array(fasttext_vectors)

random_forest = RandomForestClassifier(random_state=42)
classifiers = [('Random Forest', random_forest)]

for vec_name, vec_data in vectorizations.items():
    print(f"Vectorization: {vec_name}")
    x_train, x_test, y_train, y_test = train_test_split(vec_data, input_df['category'], random_state=42, test_size=0.25)

    for clf_name, clf in classifiers:
        clf.fit(x_train, y_train)
        pred = clf.predict(x_test)

        accuracy = accuracy_score(y_test, pred)
        precision = precision_score(y_test, pred, average='macro')
        recall = recall_score(y_test, pred, average='macro')
        f1 = f1_score(y_test, pred, average='macro')

        print(f"Classifier: {clf_name}")
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1-score: {f1:.4f}")

        cm = confusion_matrix(y_test, pred)
        print("Confusion Matrix:")
        print(cm)

        print()